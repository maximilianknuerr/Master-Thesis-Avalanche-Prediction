@online{Lawis:2022,
  author = {Lawinenwarndienst Tirol, Lawinenwarndienst Steiermark, Lawinenwarndienst Salzburg, Lawinenwarndienst Oberösterreich, Lawinenwarndienst Vorarlberg, Lawinenwarndienst Kärnten, Universität Wien},
  title = {{Lawinen Ereignisse}},
  year = 2022,
  url = {https://lawis.at/incident/},
  urldate = {2022-01-10}
}

@online{Verbund:2022,
titel = {{über Verbund}},
year = 2022,
url = {https://www.verbund.com/de-at/ueber-verbund},
urldate = {2022-04-01}
}


@online{Verbund_lawine:2022,
titel = {{VERBUND entwickelt Lawinen-Informationssystem ARIS}},
year = 2001,
url = {https://www.verbund.com/de-at/ueber-verbund/news-presse/presse/2001/03/15/aris#!/1/undefined/1/undefined/%7B%22page%22%3A0%2C%22sitepath%22%3A%22verbund%22%2C%22database%22%3A%22web%22%2C%22itemsToShow%22%3A12%2C%22country%22%3A%22Oesterreich%22%2C%22language%22%3A%22de%22%2C%22token%22%3A%22fhv7nljrs64ed2q45os8z9%22%2C%22searchword%22%3A%22lawinenwarndienst%22%2C%22facets%22%3A%5B%5D%2C%22isMobile%22%3Afalse%7D/undefined},
urldate = {2022-07-14}
}


@online{Scikit-learn-rbf-para:2022,
titel = {{RBF SVM parameters}},
year = 2022,
url = {https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html},
urldate = {2022-08-11}
}

@online{obviously.ai:2022,
titel = {{How To Know if Your Machine Learning Model Has Good Performance}},
author = {Kirsten Barkved},
year = 2022,
url = {https://www.obviously.ai/post/machine-learning-model-performance},
urldate = {2022-08-17}
}


@online{Scikit-learn-decision-tree:2022,
titel = {{Decision Trees}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/tree.html},
urldate = {2022-04-13}
}

@online{Scikit-learn-decision-tree-classifier:2022,
titel = {{sklearn.tree.DecisionTreeClassifier}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html},
urldate = {2022-07-20}
}

@online{Scikit-learn-svc:2022,
titel = {{sklearn.svm.SVC}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html},
urldate = {2022-07-25}
}

@online{Scikit-learn-confusion-matrix:2022,
titel = {{sklearn.metrics.ConfusionMatrixDisplay}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html},
urldate = {2022-08-01}
}

@online{Scikit-model-evaluation:2022,
titel = {{3.3. Metrics and scoring: quantifying the quality of predictions}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/model_evaluation.html},
urldate = {2022-08-05}
}


@online{Scikit-learn-roc-curve:2022,
titel = {{sklearn.metrics.roc_curve}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html},
urldate = {2022-07-26}
}


@online{Scikit-learn-cross-val-score:2022,
titel = {{sklearn.model_selection.cross_val_score}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html},
urldate = {2022-07-26}
}


@online{Scikit-learn-lda:2022,
titel = {{sklearn.discriminant_analysis.LinearDiscriminantAnalysis}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html},
urldate = {2022-07-25}
}

@online{Scikit-learn-train-test-split:2022,
titel = {{sklearn.model_selection.train_test_split}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html},
urldate = {2022-08-01}
}

@online{Scikit-learn-svm:2022,
titel = {{Support Vector Machines}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/svm.html},
urldate = {2022-05-24}
}

@online{Scikit-learn-svm:2022,
titel = {{Support Vector Machines}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/svm.html},
urldate = {2022-05-24}
}


@online{Scikit-learn-logistic-regression:2022,
titel = {{sklearn.linear_model.LogisticRegression}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression},
urldate = {2022-05-21}
}
@online{ibm-logistic-regression:2022,
titel = {{What is logistic regression?}},
year = 2022,
url = {https://www.ibm.com/topics/logistic-regression},
urldate = {2022-05-21}
}

@online{ibm-overfitting:2022,
titel = {{Overfitting}},
year = 2022,
url = {https://www.ibm.com/cloud/learn/overfitting},
urldate = {2022-06-07}
}

@online{ibm-supervised-learning:2022,
titel = {{Supervised Learning}},
year = 2022,
url = {https://www.ibm.com/cloud/learn/supervised-learning},
urldate = {2022-06-13}
}

@online{ibm-Nearest-Neighbors:2022,
titel = {{K-Nearest Neighbors Algorithm}},
year = 2022,
url = {https://www.ibm.com/topics/knn},
urldate = {2022-06-24}
}

@online{ionos-reinforcement-learning:2022,
titel = {{Reinforcement Learning – wenn Maschinen Denken lernen}},
year = 2022,
url = {https://www.ionos.at/digitalguide/online-marketing/suchmaschinenmarketing/was-ist-reinforcement-learning/},
urldate = {2022-06-13}
}

@online{VerbundKaprun:2022,
titel = {{Verbund Kaprun}},
year = 2022,
url = {https://www.verbund.com/de-at/ueber-verbund/besucherzentren/kaprun},
urldate = {2022-04-05}
}

@online{DeepAI:2022,
titel = {{What is a Random Forest?}},
author = {Thomas Wood},
year = 2022,
url = {https://deepai.org/machine-learning-glossary-and-terms/random-forest},
urldate = {2022-06-20}
}

@online{wyssenavalanche:2022,
titel = {{IDA® INFRASOUND DETECTION SYSTEM}},
year = 2022,
url = {https://www.wyssenavalanche.com/en/avalanche-detection/ida-infrasound-detection-system/},
urldate = {2022-06-24}
}

@online{dwd:2022,
titel = {{Meteorologie}},
year = 2022,
url = {https://www.dwd.de/DE/service/lexikon/Functions/glossar.html?lv2=101640&lv3=101744},
urldate = {2022-06-24}
}

@online{wortbedeutung_topografie:2022,
titel = {{Topografie}},
year = 2022,
url = {https://www.wortbedeutung.info/Topografie/},
urldate = {2022-06-24}
}


@online{Pragati_Preprocessing:2022,
titel = {{A Simple Guide to Data Preprocessing in Machine Learning}},
author = {Pragati Baheti},
year = 2022,
url = {https://www.v7labs.com/blog/data-preprocessing-guide},
urldate = {2022-07-25}
}



@online{analyticsvidhya_evaluation:2022,
titel = {{11 Important Model Evaluation Metrics for Machine Learning Everyone should know}},
author = {Tavish Srivastava},
year = 2019,
url = {https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/},
urldate = {2022-07-07}
}


@online{jeremyjordan_evaluation:2022,
titel = {{Evaluating a machine learning model.}},
author = {JEREMY JORDAN},
year = 2017,
url = {https://www.jeremyjordan.me/evaluating-a-machine-learning-model/},
urldate = {2022-07-07}
}

@online{Kartik_evaluation:2022,
titel = {{Various ways to evaluate a machine learning model’s performance}},
author = {Kartik Nighania},
year = 2018,
url = {https://towardsdatascience.com/various-ways-to-evaluate-a-machine-learning-models-performance-230449055f15},
urldate = {2022-07-07}
}

@online{Google_ROC_AUC:2022,
titel = {{Classification: ROC Curve and AUC}},
year = 2020,
url = {https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc},
urldate = {2022-07-08}
}

@online{Google_Acurracy:2022,
titel = {{Classification: Accuracy}},
year = 2020,
url = {https://developers.google.com/machine-learning/crash-course/classification/accuracy},
urldate = {2022-07-08}
}

@online{Google_Precision_Recall:2022,
titel = {{Classification: Precision and Recall}},
year = 2020,
url = {https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall},
urldate = {2022-07-08}
}

@online{analyticsvidhya_cross_validation:2022,
titel = {{K-Fold Cross Validation Technique and its Essentials}},
author = {Shanthababu Pandian },
year = 2022,
url = {https://www.analyticsvidhya.com/blog/2022/02/k-fold-cross-validation-technique-and-its-essentials/},
urldate = {2022-07-08}
}


@online{Sklearn_genetic_feature_docu:2022,
titel = {{FeatureSelectionCV}},
year = 2022,
url = {https://sklearn-genetic-opt.readthedocs.io/en/stable/api/featureselectioncv.html},
urldate = {2022-07-18}
}

@online{Sklearn_genetic_feature_docu_metrics:2022,
titel = {{How to Use Sklearn-genetic-opt}},
year = 2022,
url = {https://sklearn-genetic-opt.readthedocs.io/en/latest/tutorials/basic_usage.html},
urldate = {2022-07-21}
}


@online{Sklearn_StandardScaler:2022,
titel = {{sklearn.preprocessing.StandardScaler}},
year = 2022,
url = {https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn-preprocessing-standardscaler},
urldate = {2022-07-25}
}



@article{MANDREKAR20101315,
	abstract = {The performance of a diagnostic test in the case of a binary predictor can be evaluated using the measures of sensitivity and specificity. However, in many instances, we encounter predictors that are measured on a continuous or ordinal scale. In such cases, it is desirable to assess performance of a diagnostic test over the range of possible cutpoints for the predictor variable. This is achieved by a receiver operating characteristic (ROC) curve that includes all the possible decision thresholds from a diagnostic test result. In this brief report, we discuss the salient features of the ROC curve, as well as discuss and interpret the area under the ROC curve, and its utility in comparing two different tests or predictor variables of interest.},
	author = {Jayawant N. Mandrekar},
	doi = {https://doi.org/10.1097/JTO.0b013e3181ec173d},
	issn = {1556-0864},
	journal = {Journal of Thoracic Oncology},
	keywords = {Sensitivity, Specificity, ROC, AUC},
	number = {9},
	pages = {1315-1316},
	title = {Receiver Operating Characteristic Curve in Diagnostic Test Assessment},
	url = {https://www.sciencedirect.com/science/article/pii/S1556086415306043},
	volume = {5},
	year = {2010},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1556086415306043},
	bdsk-url-2 = {https://doi.org/10.1097/JTO.0b013e3181ec173d}}




@article{kotsiantis2006data,
  title={Data preprocessing for supervised leaning},
  author={Kotsiantis, Sotiris B and Kanellopoulos, Dimitris and Pintelas, Panagiotis E},
  journal={International journal of computer science},
  volume={1},
  number={2},
  pages={111--117},
  year={2006},
  publisher={Citeseer}
}



@inbook{Refaeilzadeh2009,
	address = {Boston, MA},
	author = {Refaeilzadeh, Payam and Tang, Lei and Liu, Huan},
	booktitle = {Encyclopedia of Database Systems},
	doi = {10.1007/978-0-387-39940-9_565},
	editor = {LIU, LING and {\"O}ZSU, M. TAMER},
	isbn = {978-0-387-39940-9},
	pages = {532--538},
	publisher = {Springer US},
	title = {Cross-Validation},
	url = {https://doi.org/10.1007/978-0-387-39940-9_565},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/978-0-387-39940-9_565}}



@article{nhess-2021-106,
	author = {Chawla, M. and Singh, A.},
	doi = {10.5194/nhess-2021-106},
	journal = {Natural Hazards and Earth System Sciences Discussions},
	pages = {1--18},
	title = {A data efficient machine learning model for autonomous operational avalanche forecasting},
	url = {https://nhess.copernicus.org/preprints/nhess-2021-106/},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://nhess.copernicus.org/preprints/nhess-2021-106/},
	bdsk-url-2 = {https://doi.org/10.5194/nhess-2021-106}}




@article{WEN2022103535,
	abstract = {Determining the snow avalanche-prone areas is fundamental for risk mitigation in the snowy mountains of the Qinghai-Tibet Plateau, especially in the context of current climate warming. We use the Parlung Tsangpo catchment in southeastern Tibet as an example, collect 381 snow avalanches through field surveys. On this basis, two sample sets, three combinations of conditioning factor, and four machine learning algorithms involving Support vector machine (SVM), K-nearest neighbors (KNN), Classification and Regression Tree (CART), and Multilayer perceptron (MLP) are compared to produce snow avalanche susceptibility maps. The Kappa coefficient and ROC curve are used for the accuracy test. The results show that four basic models all have good prediction capability, and the most robust one is SVM, while the comparative sample set and two comparative combinations of factors underperform. The high snow avalanche-prone areas are mainly distributed at the Ranwu-Yupu section along the National Road 318 in the upper narrow valley section of Parlung Tsangpo River, the ridges of the mountains on both sides of the middle and lower reaches, and the narrow valley sections of the tributaries. The finding not only verifies the applicability of these four machine learning algorithms in the snow avalanche susceptibility mapping, but also points out the key areas for the local snow avalanche disaster reduction and prevention. This study can provide a good reference for other snowy mountainous regions lacking well-documents of snow avalanches.},
	author = {Hong Wen and Xiyong Wu and Xin Liao and Dong Wang and Kaiyang Huang and Bernd W{\"u}nnemann},
	doi = {https://doi.org/10.1016/j.coldregions.2022.103535},
	issn = {0165-232X},
	journal = {Cold Regions Science and Technology},
	keywords = {Snow avalanche, Susceptibility mapping, Machine learning methods, Parlung Tsangpo cathment},
	pages = {103535},
	title = {Application of machine learning methods for snow avalanche susceptibility mapping in the Parlung Tsangpo catchment, southeastern Qinghai-Tibet Plateau},
	url = {https://www.sciencedirect.com/science/article/pii/S0165232X22000544},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0165232X22000544},
	bdsk-url-2 = {https://doi.org/10.1016/j.coldregions.2022.103535}}


@article{Tharwat:2017,
	abstract = {Linear Discriminant Analysis (LDA) is a very common technique for dimensionality reduction problems as a pre-processing step for machine learning and pattern classification applications. At the same time, it is usually used as a black box, but (sometimes) not well understood. The aim of this paper is to build a solid intuition for what is LDA, and how LDA works, thus enabling readers of all levels be able to get a better understanding of the LDA and to know how to apply this technique in different applications. The paper first gave the basic definitions and steps of how LDA technique works supported with visual explanations of these steps. Moreover, the two methods of computing the LDA space, i.e. class-dependent and class-independent methods, were explained in details. Then, in a step-by-step approach, two numerical examples are demonstrated to show how the LDA space can be calculated in case of the class-dependent and class-independent methods. Furthermore, two of the most common LDA problems (i.e. Small Sample Size (SSS ) and non-linearity problems) were highlighted and illustrated, and state-of-the-art solutions to these problems were investigated and explained. Finally, a number of experiments was conducted with different datasets to (1) investigate the effect of the eigenvectors that used in the LDA space on the robustness of the extracted feature for the classification accuracy, and (2) to show when the SSS problem occurs and how it can be addressed.},
	author = {Tharwat, Alaa and Gaber, Tarek and Ibrahim, Abdelhameed and Hassanien, Aboul Ella},
	date-added = {2022-06-15 22:08:17 +0200},
	date-modified = {2022-06-15 22:08:17 +0200},
	doi = {10.3233/AIC-170729},
	isbn = {1875-8452},
	journal = {AI Communications},
	keywords = {Dimensionality reduction; PCA; LDA; Kernel Functions; Class-Dependent LDA; Class-Independent LDA; SSS (Small Sample Size) problem; eigenvectors artificial intelligence},
	number = {2},
	pages = {169--190},
	publisher = {IOS Press},
	title = {Linear discriminant analysis: A detailed tutorial},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.3233/AIC-170729}}



@inbook{Xanthopoulos2013,
	abstract = {In this chapter we discuss another popular data mining algorithm that can be used for supervised or unsupervised learning. Linear Discriminant Analysis (LDA) was proposed by R. Fischer in 1936. It consists in finding the projection hyperplane that minimizes the interclass variance and maximizes the distance between the projected means of the classes. Similarly to PCA, these two objectives can be solved by solving an eigenvalue problem with the corresponding eigenvector defining the hyperplane of interest. This hyperplane can be used for classification, dimensionality reduction and for interpretation of the importance of the given features. In the first part of the chapter we discuss the generic formulation of LDA whereas in the second we present the robust counterpart scheme originally proposed by Kim and Boyd. We also discuss the non linear extension of LDA through the kernel transformation.},
	address = {New York, NY},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	booktitle = {Robust Data Mining},
	doi = {10.1007/978-1-4419-9878-1_4},
	isbn = {978-1-4419-9878-1},
	pages = {27--33},
	publisher = {Springer New York},
	title = {Linear Discriminant Analysis},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_4},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4419-9878-1_4}}



@article{analyticsvidhyaLDA:2021,
titel = {{A Brief Introduction to Linear Discriminant Analysis}},
author = {Sunil Kumar Dash},
year = 2021,
journal = {Data Science Blogathon},
volume = 10,
url = {https://www.analyticsvidhya.com/blog/2021/08/a-brief-introduction-to-linear-discriminant-analysis/},
urldate = {2022-04-05}
}



@article{DEMIR2005421,
	abstract = {Online local learning algorithms for a laterally-connected single-layer neural network for performing linear discriminant analysis have been proposed. A convergence proof is provided for the algorithm based on Hebbian learning. The algorithms are simulated and applied to the face recognition problem.},
	author = {G.K. Demir and K. Ozmehmet},
	doi = {https://doi.org/10.1016/j.patrec.2004.08.005},
	issn = {0167-8655},
	journal = {Pattern Recognition Letters},
	keywords = {Linear discriminant analysis, Generalized eigenvalue problem, Online local learning},
	note = {ICAPR 2003},
	number = {4},
	pages = {421-431},
	title = {Online local learning algorithms for linear discriminant analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865504001825},
	volume = {26},
	year = {2005},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167865504001825},
	bdsk-url-2 = {https://doi.org/10.1016/j.patrec.2004.08.005}}


@incollection{MENDLEIN2013646,
	abstract = {Chemometrics is a family of multivariate statistical methods that are applied to chemical data. Although well established in the chemistry community, chemometrics is still gaining popularity in the forensic science community. This article first addresses the need for preprocessing of data to remove any systematic but uninformative components to the data. Then, the background, basis, and use of three popular chemometric algorithms (agglomerative hierarchical clustering, principal components analysis, and discriminant analysis) are described. Several references to the forensic applications of these techniques are included along with electronic and printed resources.},
	address = {Waltham},
	author = {A. Mendlein and C. Szkudlarek and J.V. Goodpaster},
	booktitle = {Encyclopedia of Forensic Sciences (Second Edition)},
	doi = {https://doi.org/10.1016/B978-0-12-382165-2.00259-2},
	edition = {Second Edition},
	editor = {Jay A. Siegel and Pekka J. Saukko and Max M. Houck},
	isbn = {978-0-12-382166-9},
	keywords = {Agglomerative hierarchical clustering (AHC), Chemometrics, Cluster analysis, Clustering, Data analysis, Linear discriminant analysis (LDA), Pattern recognition, Principal components analysis (PCA)},
	pages = {646-651},
	publisher = {Academic Press},
	title = {Chemometrics},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123821652002592},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780123821652002592},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-382165-2.00259-2}}


@incollection{GHOLAMI2017515,
	abstract = {Support Vector Machine (SVM) has been introduced in the late 1990s and successfully applied to many engineering related applications. In this chapter, attempts were made to introduce the SVM, its principles, structures, and parameters. The issue of selecting a kernel function and other associated parameters of SVMs was also raised and applications from different petroleum and mining related tasks were brought to show how those parameters can be properly selected. It seems that the cross-validation approach would be the best technique for parameter selections of SVMs but few other concerns such as running time must not be neglected.},
	author = {Raoof Gholami and Nikoo Fakhari},
	booktitle = {Handbook of Neural Computation},
	doi = {https://doi.org/10.1016/B978-0-12-811318-9.00027-2},
	editor = {Pijush Samui and Sanjiban Sekhar and Valentina E. Balas},
	isbn = {978-0-12-811318-9},
	keywords = {Support Vector Machine, Generalization, Kernels, Trade-off parameter, Applications},
	pages = {515-535},
	publisher = {Academic Press},
	title = {Chapter 27 - Support Vector Machine: Principles, Parameters, and Applications},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128113189000272},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780128113189000272},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-811318-9.00027-2}}



@incollection{VIEIRA20201,
	abstract = {Machine learning is becoming increasingly popular in the neuroscientific literature. However, navigating the literature can easily become overwhelming, especially for the nonexpert. In this chapter, we provide an introduction to machine learning aimed at researchers, clinicians, and students with an interest in brain disorders, including psychiatry and neurology. We first provide a brief overview of how the most prominent theories of human learning from the fields of psychology and neuroscience influenced the development of modern cutting-edge machine learning methods. Second, we discuss how these methods differ from classical statistics and why they could be particularly suited to the investigation of brain disorders. In the final section of this chapter, we introduce a high-level taxonomy of the main approaches used in the machine learning literature: supervised learning, unsupervised learning, semisupervised learning, and reinforcement learning.},
	author = {Sandra Vieira and Walter Hugo {Lopez Pinaya} and Andrea Mechelli},
	booktitle = {Machine Learning},
	doi = {https://doi.org/10.1016/B978-0-12-815739-8.00001-8},
	editor = {Andrea Mechelli and Sandra Vieira},
	isbn = {978-0-12-815739-8},
	keywords = {Brain disorders, Classical statistics, Generalization, Heterogeneity, Human learning, Machine learning, Neuroscience, Prediction, Psychology},
	pages = {1-20},
	publisher = {Academic Press},
	title = {Chapter 1 - Introduction to machine learning},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128157398000018},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780128157398000018},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-815739-8.00001-8}}


@incollection{VIEIRA202021,
	abstract = {The last decade has seen a surge in machine learning studies in psychiatric and neurological disorders. Given its translational potential, machine learning is also capturing the interest of clinicians and other mental health practitioners. It is therefore important that these research and clinical communities develop a good appreciation of the nature of the machine learning process. This chapter aims to provide an accessible introduction to the main steps of the supervised learning pipeline, the most prevalent type of machine learning in the neuroscientific literature. The main stages of the standard pipeline include problem formulation, data preparation, feature engineering, model training, model evaluation, and post hoc analysis. Building a successful model is often an iterative process of adjustment of these several components. A good understanding of the rationale and challenges of each one is essential to avoid spurious interpretations.},
	author = {Sandra Vieira and Walter Hugo {Lopez Pinaya} and Andrea Mechelli},
	booktitle = {Machine Learning},
	doi = {https://doi.org/10.1016/B978-0-12-815739-8.00002-X},
	editor = {Andrea Mechelli and Sandra Vieira},
	isbn = {978-0-12-815739-8},
	keywords = {Classification, Cross-validation, Feature engineering, Model evaluation, Model training, Neurological disorders, Pipeline, Psychiatric disorders, Regression, Supervised machine learning},
	pages = {21-44},
	publisher = {Academic Press},
	title = {Chapter 2 - Main concepts in machine learning},
	url = {https://www.sciencedirect.com/science/article/pii/B978012815739800002X},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B978012815739800002X},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-815739-8.00002-X}}

@incollection{PISNER2020101,
	abstract = {In this chapter, we explore Support Vector Machine (SVM)---a machine learning method that has become exceedingly popular for neuroimaging analysis in recent years. Because of their relative simplicity and flexibility for addressing a range of classification problems, SVMs distinctively afford balanced predictive performance, even in studies where sample sizes may be limited. In brain disorders research, SVMs are typically employed using multivoxel pattern analysis (MVPA) because their relative simplicity carries a lower risk of overfitting even using high-dimensional imaging data. More recently, SVMs have been used in the context of precision psychiatry, particularly for applications that involve predicting diagnosis and prognosis of brain diseases such as Alzheimer's disease, schizophrenia, and depression. In the last section of this chapter, we review a number of recent studies that use SVM for such applications.},
	author = {Derek A. Pisner and David M. Schnyer},
	booktitle = {Machine Learning},
	doi = {https://doi.org/10.1016/B978-0-12-815739-8.00006-7},
	editor = {Andrea Mechelli and Sandra Vieira},
	isbn = {978-0-12-815739-8},
	keywords = {Classification, Depression, Diagnosis, Hyperplane, Mild cognitive impairment, Multivoxel pattern analysis, Prognosis, Schizophrenia, Searchlight, Support Vector Machine},
	pages = {101-121},
	publisher = {Academic Press},
	title = {Chapter 6 - Support vector machine},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128157398000067},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780128157398000067},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-815739-8.00006-7}}


@incollection{SUBASI202091,
	abstract = {Machine learning uses the theory of statistics to build mathematical models, as the main objective is to yield inferences from a sample. Once a model is built, its representation and algorithmic solution for interpretation needs to be competent as well. In some applications, the competence of the machine learning algorithm might be as important as its classification accuracy. Machine learning is used in several fields, including forecasting, anomaly detection, and biomedical data analysis as a decision support element. The purpose of this chapter is to help scientists select an appropriate machine learning technique and guide them using optimal strategies by employing real-time databases, in addition to familiarizing readers with the basics of machine learning before taking a deep dive into solving real-world problems with machine learning techniques. Basic concepts are covered in areas such as artificial intelligence, data mining, computer science, data science, natural language processing, deep learning, mathematics, and statistics. Topics related to the various machine learning techniques will be explored, including supervised, unsupervised, and reinforcement learning, hence important machine learning algorithms are discussed in this chapter. Toward the end of every section, suitable Python functions will be illustrated as examples. Most of the examples are taken from Python--scikit-learn library (https://scikit-learn.org/stable/) and TensorFlow, and then adapted.},
	author = {Abdulhamit Subasi},
	booktitle = {Practical Machine Learning for Data Analysis Using Python},
	doi = {https://doi.org/10.1016/B978-0-12-821379-7.00003-5},
	editor = {Abdulhamit Subasi},
	isbn = {978-0-12-821379-7},
	keywords = {linear discriminant analysis (LDA), k-nearest neighbour (k-NN), artificial neural networks (ANN), support vector machines (SVM), decision tree algorithm, deep learning, clustering},
	pages = {91-202},
	publisher = {Academic Press},
	title = {Chapter 3 - Machine learning techniques},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128213797000035},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780128213797000035},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-821379-7.00003-5}}



@incollection{BELYADI2021169,
	abstract = {This chapter covers the theory, step-by-step codes, and applications of various supervised learning algorithms including multilinear regression, logistic regression, k-nearest neighbor (KNN), support vector machine (SVM), decision tree, random forest, extra trees, gradient boosting, extreme gradient boosting, and adaptive gradient boosting using scikit-learn. Various machine learning (ML) applications in the oil and gas industry, including productivity effectiveness, binary classification of ML application in geomechanical log property prediction (shear wave and compression wave travel time predictions), TOC prediction, net present value prediction, frac intensity binary classification, and rate of penetration prediction, have been detailed and illustrated in an easy workflow in Python. Finally, handling missing data using the effective impute package library in Python is discussed. K-nearest neighbor, iterative imputer, and multivariate imputation by chained equations (MICE) are illustrated with the step-by-step codes in Python. These techniques were applied to impute the missing values in a frac stage data set. The examples and guidelines shown in this chapter can be easily applied to solve various other oil and gas--related problems.},
	author = {Hoss Belyadi and Alireza Haghighat},
	booktitle = {Machine Learning Guide for Oil and Gas Using Python},
	doi = {https://doi.org/10.1016/B978-0-12-821929-4.00004-4},
	editor = {Hoss Belyadi and Alireza Haghighat},
	isbn = {978-0-12-821929-4},
	keywords = {Adaptive gradient boosting, Decision tree, Extra trees, Extreme gradient boosting, Frac intensity prediction, Geomechanical log prediction, Gradient boosting, K-nearest neighbor (KNN), Logistic regression, Missing data imputation of frac data, Multilinear regression, Net present value prediction, Productivity prediction, Random forest, ROP prediction, Support vector machine (SVM), TOC prediction},
	pages = {169-295},
	publisher = {Gulf Professional Publishing},
	title = {Chapter 5 - Supervised learning},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128219294000044},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780128219294000044},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-821929-4.00004-4}}





@article{Sourav:2020,
abstract = {Linear Regression is a commonly used supervised Machine Learning algorithm that predicts continuous values. Linear Regression assumes that there is a linear relationship present between dependent and independent variables. In simple words, it finds the best fitting line/plane that describes two or more variables.

On the other hand, Logistic Regression is another supervised Machine Learning algorithm that helps fundamentally in binary classification (separating discreet values).

Although the usage of Linear Regression and Logistic Regression algorithm is completely different, mathematically we can observe that with an additional step we can convert Linear Regression into Logistic Regression.},
title = {{Beginners Take: How Logistic Regression is related to Linear Regression}},
journal = {Data Science Blogathon},
year = 2020,
month = dez,
  day = 1,
url = {https://www.analyticsvidhya.com/blog/2020/12/beginners-take-how-logistic-regression-is-related-to-linear-regression/},
}

@article{NUSINOVICI202056,
	abstract = {Objective
To evaluate the performance of machine learning (ML) algorithms and to compare them with logistic regression for the prediction of risk of cardiovascular diseases (CVDs), chronic kidney disease (CKD), diabetes (DM), and hypertension (HTN) and in a prospective cohort study using simple clinical predictors.
Study Design and Setting
We conducted analyses in a population-based cohort study in Asian adults (n = 6,762). Five different ML models were considered---single-hidden-layer neural network, support vector machine, random forest, gradient boosting machine, and k-nearest neighbor---and were compared with standard logistic regression.
Results
The incidences at 6 years of CVD, CKD, DM, and HTN cases were 4.0%, 7.0%, 9.2%, and 34.6%, respectively. Logistic regression reached the highest area under the receiver operating characteristic curve for CKD (0.905 [0.88, 0.93]) and DM (0.768 [0.73, 0.81]) predictions. For CVD and HTN, the best models were neural network (0.753 [0.70, 0.81]) and support vector machine (0.780 [0.747, 0.812]), respectively. However, the differences with logistic regression were small (less than 1%) and nonsignificant. Logistic regression, gradient boosting machine, and neural network were systematically ranked among the best models.
Conclusion
Logistic regression yields as good performance as ML models to predict the risk of major chronic diseases with low incidence and simple clinical predictors.},
	author = {Simon Nusinovici and Yih Chung Tham and Marco Yu {Chak Yan} and Daniel Shu {Wei Ting} and Jialiang Li and Charumathi Sabanayagam and Tien Yin Wong and Ching-Yu Cheng},
	doi = {https://doi.org/10.1016/j.jclinepi.2020.03.002},
	issn = {0895-4356},
	journal = {Journal of Clinical Epidemiology},
	keywords = {Machine learning, Logistic regression, Prognostic modeling, Chronic diseases, Interaction, Nonlinearity},
	pages = {56-69},
	title = {Logistic regression was as good as machine learning for predicting major chronic diseases},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435619310194},
	volume = {122},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0895435619310194},
	bdsk-url-2 = {https://doi.org/10.1016/j.jclinepi.2020.03.002}}



@article{LU2008887,
	abstract = {Machine learning techniques for feature selection, which include the optimization of feature descriptor weights and the selection of optimal feature descriptor subset, are desirable to enhance the performance of image annotation systems. In our system, the multimedia content description interface (MPEG-7) image feature descriptors consisting of color descriptors, texture descriptors and shape descriptors are employed to represent low-level image features. We use a real coded chromosome genetic algorithm and k-nearest neighbor (k-NN) classification accuracy as fitness function to optimize the weights of MPEG-7 image feature descriptors. A binary one and k-NN classification accuracy combining with the size of feature descriptor subset as fitness function are used to select optimal MPEG-7 feature descriptor subset. Furthermore, a bi-coded chromosome genetic algorithm is used for the simultaneity of weight optimization and descriptor subset selection, whose fitness function is the same as that of the binary one. The experimental results over 2000 classified Corel images show that with the real coded genetic algorithm, the binary coded one and the bi-coded one, the accuracies of image annotation system are improved by 7%, 9% and 13.6%, respectively, comparing to the method without machine learning. Furthermore, 2 of 25 MPEG-7 feature descriptors are selected with the binary coded genetic algorithm and four with the bi-coded one, which may improve the efficiency of system significantly.},
	author = {Jianjiang Lu and Tianzhong Zhao and Yafei Zhang},
	doi = {https://doi.org/10.1016/j.knosys.2008.03.051},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Image annotation, Feature selection, Genetic algorithm, -Nearest neighbor classifier, Multimedia content description interface},
	number = {8},
	pages = {887-891},
	title = {Feature selection based-on genetic algorithm for image annotation},
	url = {https://www.sciencedirect.com/science/article/pii/S095070510800097X},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070510800097X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2008.03.051}}



@article{Yang:2018,
	author = {Tao-Chang Yang, Pao-Shan Yu, Kun-Hsiang Lin, Chen-Min Kuo, Hung-Wei Tseng},
	title = {Predictor selection method for the construction of support vector machine (SVM)-based typhoon rainfall forecasting models using a non-dominated sorting genetic algorithm},
	abstract = {This study proposes a predictor selection method for constructing a support vector machine (SVM)-based typhoon rainfall forecasting models using a fast elitist Non-dominated Sorting Genetic Algorithm II (NSGA-II). Based on SVMs, four rainfall forecasting models with different combinations of the three types of input variables (i.e. antecedent rainfalls, typhoon characteristics and local weather factors) were constructed for 1–6 hr-ahead forecasting. An application to three rain gauge stations in the Yilan River basin, northeastern Taiwan, was conducted to demonstrate the superiority of the proposed predictor selection method. The results showed that the optimal combination of predictors for each SVM-based rainfall forecasting model can be automatically and effectively determined by the proposed predictor selection method. The rainfall forecasting model using all three types of input variables performed better than the other three models, especially for long lead-time forecasting. The construction of rainfall forecasting models is helpful to extend the lead time of flood forecasting. The optimal rainfall forecasting model can be further integrated with river hydraulic models or flood inundation models for flood forecasting to assist floodplain managers to take suitable precautionary measures during typhoon landfall.},
	journal = {Meteorological Applications},
	volume = {25},
	year = {2018},
	doi = {https://doi.org/10.1002/met.1717},
	url = {https://rmets.onlinelibrary.wiley.com/doi/10.1002/met.1717}

}


@article{DBLP:1912,
  author    = {Petro Liashchynskyi and
               Pavlo Liashchynskyi},
  title     = {Grid Search, Random Search, Genetic Algorithm: {A} Big Comparison
               for {NAS}},
  journal   = {CoRR},
  volume    = {abs/1912.06059},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.06059},
  eprinttype = {arXiv},
  eprint    = {1912.06059},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-06059.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VenkateshAnuradha:2019,
	author = {B. Venkatesh and J. Anuradha},
	doi = {doi:10.2478/cait-2019-0001},
	journal = {Cybernetics and Information Technologies},
	number = {1},
	pages = {3--26},
	title = {A Review of Feature Selection and Its Methods},
	url = {https://doi.org/10.2478/cait-2019-0001},
	volume = {19},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.2478/cait-2019-0001}}

@article{ALLAM2022329,
	abstract = {Feature selection is a significant task in the workflow of predictive modeling for data analysis. Recent advanced feature selection methods are using the power of optimization algorithms for choosing a subset of relevant features to get better classification results. Most of the optimization algorithms like genetic algorithm use many controlling parameters which need to be tuned for better performance. Tuning these parameter values is a challenging task for the feature selection process. In this paper, we have developed a new wrapper-based feature selection method called binary teaching learning based optimization (FS-BTLBO) algorithm which needs only common controlling parameters like population size, and a number of generations to obtain a subset of optimal features from the dataset. We have used different classifiers as an objective function to compute the fitness of individuals for evaluating the efficiency of the proposed system. The results have proven that FS-BTLBO produces higher accuracy with a minimal number of features on Wisconsin diagnosis breast cancer (WDBC) data set to classify malignant and benign tumors.},
	author = {Mohan Allam and M. Nandhini},
	doi = {https://doi.org/10.1016/j.jksuci.2018.12.001},
	issn = {1319-1578},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	keywords = {Feature selection, Binary teaching learning based optimization, Genetic algorithm, Breast cancer},
	number = {2},
	pages = {329-341},
	title = {Optimal feature selection using binary teaching learning based optimization algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157818306463},
	volume = {34},
	year = {2022},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S1319157818306463},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jksuci.2018.12.001}}


@article{SUGUMARAN2007930,
	abstract = {Roller bearing is one of the most widely used rotary elements in a rotary machine. The roller bearing's nature of vibration reveals its condition and the features that show the nature, are to be extracted through some indirect means. Statistical parameters like kurtosis, standard deviation, maximum value, etc. form a set of features, which are widely used in fault diagnostics. Often the problem is, finding out good features that discriminate the different fault conditions of the bearing. Selection of good features is an important phase in pattern recognition and requires detailed domain knowledge. This paper illustrates the use of a Decision Tree that identifies the best features from a given set of samples for the purpose of classification. It uses Proximal Support Vector Machine (PSVM), which has the capability to efficiently classify the faults using statistical features. The vibration signal from a piezoelectric transducer is captured for the following conditions: good bearing, bearing with inner race fault, bearing with outer race fault, and inner and outer race fault. The statistical features are extracted therefrom and classified successfully using PSVM and SVM. The results of PSVM and SVM are compared.},
	author = {V. Sugumaran and V. Muralidharan and K.I. Ramachandran},
	doi = {https://doi.org/10.1016/j.ymssp.2006.05.004},
	issn = {0888-3270},
	journal = {Mechanical Systems and Signal Processing},
	keywords = {Feature selection, Decision Tree, Roller bearing, Statistical features, PSVM, Fault detection},
	number = {2},
	pages = {930-942},
	title = {Feature selection using Decision Tree and classification through Proximal Support Vector Machine for fault diagnostics of roller bearing},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327006001142},
	volume = {21},
	year = {2007},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0888327006001142},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ymssp.2006.05.004}}


@article{Martin:2001,
abstract = {The SAFRAN/Crocus/MÉPRA software is used to assess the climatology of the avalanche hazard and its sensitivity to climate change. A natural avalanche-hazard index based on MEPRA analysis is defined and validated against natural avalanche observations (triggered avalanches are not taken into account). A 15 year climatology then allows a comparison of avalanche hazard in the different French massifs. Finally a simple climate scenario (with a general increase of precipitation and temperature) shows that avalanche hazard may decrease slightly in winter (mainly February) and more significantly in May/June. The relative proportion of wet-snow avalanches increases.},
author = {Eric Martin, Gérald Giraud, Yves Lejeune, Géraldine Boudart},
journal = {Annals of Glaciology},
pages = {163--167},
volume = {32},
title = {{Impact of a climate change on avalanche hazard}},
year = {2001},
url = {https://doi.org/10.3189/172756401781819292}
}


@article{Bahram:2019,
abstract = {Snow avalanches are among the most destructive natural hazards threatening human life, ecosystems, built structures, and landscapes in mountainous regions. The complexity of snow avalanche modelling has been discussed in many studies, but its modelling is not well-documented. Snow avalanche modeling in this study was done using three main categories of data, including avalanche occurrence locations, meteorological factors, and terrain characteristics. Two machine learning models, namely support vector machine (SVM) and multivariate discriminant analysis (MDA), were employed. A ratio of 70 to 30 of data was considered for calibrating and validating the models. Results indicated that both models had an excellent performance in snow avalanche modeling (area under curve, AUC > 90), although hits and misses analysis demonstrated the superior performance of MDA. Sensitivity analysis indicated that the topographic position index, slope, precipitation, and topographic wetness index were the most effective variables for modeling. A snow avalanche map indicated that the high snow avalanche hazard zone was mostly near the streams and was matched with hillsides around the water pathways. Findings of study can be helpful for land use planning, to control snow avalanche paths, and to prevent the probable hazards induced by it, and it can be a good reference for future studies on modeling snow avalanche hazards.},
author = {Bahram Choubin, Moslem Borji, Amir Mosavi, Farzaneh Sajedi-Hosseini, Vijay P.Singh, Shahaboddin Shamshirband},
journal = {Journal of Hydrology},
volume = {577},
title = {{Snow avalanche hazard prediction using machine learning methods}},
year = {2019},
url = {https://doi.org/10.1016/j.jhydrol.2019.123929}
}

@article{Tiwari:2021,
abstract = {Due to ongoing climate change, water mass redistribution and related hazards are getting stronger and frequent. Therefore, predicting extreme hydrological events and related hazards is one of the highest priorities in geosciences. Machine Learning (ML) methods have shown promising prospects in this venture. Every ML method requires training where we know both the output (extreme event) and input (relevant physical parameters and variables). This step is critical to the efficacy of the ML method. The usual approach is to include a wide variety of hydro-meteorological observations and physical parameters, but recent advances in ML indicate that the efficacy of ML may not improve by increasing the number of input parameters. In fact, including unimportant parameters decreases the efficacy of ML algorithms. Therefore, it is imperative that the most relevant parameters are identified prior to training. In this study, we demonstrate this concept by predicting avalanche susceptibility in Leh-Manali highway (one of the most severely affected regions in India) with and without Parameter Importance Assessment (PIA). The avalanche locations were randomly divided into two groups: 70% for training and 30% for testing. Then, based on temporal and spatial sensor data, eleven avalanche influencing parameters were considered. The Boruta algorithm, an extension of Random Forest (RF) ML method that utilizes the importance measure to rank predictors, was used and it found nine out of eleven parameters to be important. Support Vector Machine (SVM) based ML technique is used for avalanche prediction, and to be comprehensive, four different kernel functions were employed (linear, polynomial, sigmoid, and radial basis function (RBF)). The prediction accuracy for linear, polynomial, sigmoid, and RBF kernels, with all the eleven parameters were found to be 80.4%, 81.7%, 39.2%, and 85.7%, respectively. While, when using selected parameters, the prediction accuracy for linear, polynomial, sigmoid, and RBF kernels were 84.1%, 86.6%, 43.0%, and 87.8%, respectively. We also identified locations where occurrences of avalanches are most likely. We conclude that parameter selection should be considered when applying ML methods in geosciences.},
author = {Anuj Tiwari, Arun G., Bramha Dutt, Vishwakarma},
journal = {Science of The Total Environment},
volume = {794},
title = {Parameter importance assessment improves efficacy of machine learning methods for predicting snow avalanche sites in Leh-Manali Highway, India},
year = {2021},
url = {https://doi.org/10.1016/j.scitotenv.2021.148738}
}

@article{THURING201560,
title = {Robust snow avalanche detection using supervised machine learning with infrasonic sensor arrays},
journal = {Cold Regions Science and Technology},
volume = {111},
pages = {60-66},
year = {2015},
issn = {0165-232X},
doi = {https://doi.org/10.1016/j.coldregions.2014.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0165232X14002419},
author = {Thomas Thüring and Marcel Schoch and Alec {van Herwijnen} and Jürg Schweizer},
keywords = {Snow avalanches, Infrasound, Array processing, Automated monitoring, Machine learning},
abstract = {Automated detection of snow avalanches is crucial to assess the effectiveness of avalanche control by explosions, and to monitor avalanche activity in a given area in view of avalanche forecasting. Several automated or semi-automated detection technologies have been developed in the past among which infrasound-based detection is the most promising for regional-scale avalanche monitoring. However, due to significant ambient noise content in infrasonic signals, e.g. from atmospheric processes or airplanes, fully automated and reliable avalanche detection has been very challenging. Signal processing is highly critical and strongly affects detection accuracy. Here, a robust detection method by using supervised machine learning is introduced. Machine learning algorithms can take into account multiple signal features and statistically optimize the classification task. We analyzed infrasound data with concurrent visual avalanche observations from the test site Lavin (Eastern Swiss Alps) for the winter of 2011–2012. A support vector machine was trained by using training data from the first half of the winter season and the accuracy was tested on data from the second half of the season. A significant reduction of false detections, from 65% to 10%, was achieved compared to a threshold-based classifier provided by the sensor manufacturer. The proposed method enables reliable assessment of the avalanche activity in the surroundings of the system and paves the way towards robust and fully automated avalanche detection using infrasonic systems.}
}

@article{Lawine:2019,
url = {https://austria-forum.org/af/AustriaWiki/Lawine?version=3}, 
author = {Verified by Hermann Maurer}, 
institution = {TU Graz},
version = {3},
urldate = {2022-01-15},
year = {2019}
}






@article{CAI201870,
title = {Feature selection in machine learning: A new perspective},
journal = {Neurocomputing},
volume = {300},
pages = {70-79},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.11.077},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218302911},
author = {Jie Cai and Jiawei Luo and Shulin Wang and Sheng Yang},
keywords = {Feature selection, Dimensionality reduction, Machine learning, Data mining},
abstract = {High-dimensional data analysis is a challenge for researchers and engineers in the fields of machine learning and data mining. Feature selection provides an effective way to solve this problem by removing irrelevant and redundant data, which can reduce computation time, improve learning accuracy, and facilitate a better understanding for the learning model or data. In this study, we discuss several frequently-used evaluation measures for feature selection, and then survey supervised, unsupervised, and semi-supervised feature selection methods, which are widely applied in machine learning problems, such as classification and clustering. Lastly, future challenges about feature selection are discussed.}
}



@article{Harvey:2016,
title = {Statistical Nowcast of Avalanche Activity at the Regional Scale},
author = {Stephan Harvey, Alec van Herwijnen, Bettina Richter},
year = {2016},
abstract = {Forecasting natural avalanche activity is challenging. Events are relatively rare and several contributory factors affecting the formation of dry- or wet-snow avalanches have to be evaluated. In contrast to previous statistical avalanche forecast models which focus on specific avalanche types or small scales, in this study we developed a model to predict avalanche days for an entire winter season at the regional scale. We therefore analysed an avalanche catalogue consisting of observed avalanches in the region of Davos, Switzerland, over the last 13 years (2003/04 to 2015/16). In combination with data from an automatic weather station and simulated snow cover properties from the model SNOWPACK, we trained random forest models by applying different methods to predict avalanche days. Overall, the predictive performance was in line with other similar studies. However, substantial differences in performance were observed among the 13 winter seasons. Surprisingly, the performance of models without snow cover data from the SNOWPACK simulations was very similar. These results suggest that there is no strong correlation between avalanche activity and snow cover properties, highlighting the limitations of obtained avalanche activity data through visual observations. While more work is still required, the reasonable performance of our statistical model to predict avalanche days show that automatically predicting regional avalanche activity using an automatic weather stations is feasible.},
url = {https://arc.lib.montana.edu/snow-science/item/2437}
}


@article{Pozdnoukhov:2008,
title={Applying machine learning methods to avalanche forecasting},
abstract={Avalanche forecasting is a complex process involving the assimilation of multiple data sources to make predictions over varying spatial and temporal resolutions. Numerically assisted forecasting often uses nearest-neighbour methods (NN), which are known to have limitations when dealing with high-dimensional data. We apply support vector machines (SVMs) to a dataset from Lochaber, Scotland, UK, to assess their applicability in avalanche forecasting. SVMs belong to a family of theoretically based techniques from machine learning and are designed to deal with high-dimensional data. Initial experiments showed that SVMs gave results that were comparable with NN for categorical and probabilistic forecasts. Experiments utilizing the ability of SVMs to deal with high dimensionality in producing a spatial forecast show promise, but require further work.},
volume={49},
DOI={10.3189/172756408787814870}, 
journal={Annals of Glaciology}, 
publisher={Cambridge University Press}, 
author={Pozdnoukhov, A. and Purves, R.S. and Kanevski, M.}, 
year={2008},
pages={107–113}
}













